{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from neo4j import GraphDatabase\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEO4J_URI=\"bolt://localhost\"\n",
    "NEO4J_USERNAME=\"neo4j\"\n",
    "NEO4J_PASSWORD=\"neo4jneo4j\"\n",
    "NEO4J_DATABASE=\"neo4j\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "driver = GraphDatabase.driver(NEO4J_URI, auth=(NEO4J_USERNAME, NEO4J_PASSWORD))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_connection():\n",
    "    try:\n",
    "        with driver.session(database=NEO4J_DATABASE) as session:\n",
    "            result = session.run(\"RETURN 1 AS result\")\n",
    "            for record in result:\n",
    "                print(\"Connection successful, returned:\", record[\"result\"])\n",
    "    except Exception as e:\n",
    "        print(\"An error occurred:\", e)\n",
    "    finally:\n",
    "        driver.close()\n",
    "\n",
    "# Test the connection\n",
    "test_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batched_import(statement, df, batch_size=1000):\n",
    "    total = len(df)\n",
    "    start_s = time.time()\n",
    "    for start in range(0,total, batch_size):\n",
    "        batch = df.iloc[start: min(start+batch_size,total)]\n",
    "        result = driver.execute_query(\"UNWIND $rows AS value \" + statement, \n",
    "                                      rows=batch.to_dict('records'),\n",
    "                                      database_=NEO4J_DATABASE)\n",
    "        print(result.summary.counters)\n",
    "    print(f'{total} rows in { time.time() - start_s} s.')    \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create constraints\n",
    "\n",
    "statements = \"\"\"\n",
    "create constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique;\n",
    "create constraint document_id if not exists for (d:__Document__) require d.id is unique;\n",
    "create constraint entity_id if not exists for (c:__Community__) require c.community is unique;\n",
    "create constraint entity_id if not exists for (e:__Entity__) require e.id is unique;\n",
    "create constraint entity_title if not exists for (e:__Entity__) require e.title is unique;\n",
    "create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique;\n",
    "\"\"\".split(\";\")\n",
    "\n",
    "for s in statements:\n",
    "    if len((s or \"\").strip()) > 0:\n",
    "        print(s)\n",
    "        driver.execute_query(query_=s,database_=NEO4J_DATABASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "GRAPHRAG_FOLDER=\"~/mp2_visualize_cypher/output/20240713-191603/artifacts\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_base_entity_graph.parquet')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import documents\n",
    "statement = \"\"\"\n",
    "MERGE (d:__Document__ {id:value.id})\n",
    "SET d += value {.title}\n",
    "// , text_unit_ids:value.text_unit_ids, raw_content:substring(value.raw_content,0,1000)};\n",
    "\"\"\"\n",
    "df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_documents.parquet', columns=[\"id\", \"title\"])\n",
    "\n",
    "batched_import(statement, df)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import text units\n",
    "statement = \"\"\"\n",
    "MERGE (c:__Chunk__ {id:value.chunk_id})\n",
    "SET c += value {.chunk, .n_tokens}\n",
    "WITH *\n",
    "UNWIND value.document_ids as doc_id\n",
    "MATCH (d:__Document__ {id:doc_id})\n",
    "MERGE (d)<-[:PART_OF]-(c)\n",
    "RETURN count(distinct c) as chunksCreated\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_base_text_units.parquet', \n",
    "                     columns=[\"chunk_id\",\"chunk\",\"n_tokens\",\"document_ids\"])\n",
    "\n",
    "batched_import(statement, df)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import nodes\n",
    "\n",
    "statement = \"\"\"\n",
    "MERGE (n:__Entity__ {id:value.id})\n",
    "SET n += value {.level, .top_level_node_id, .human_readable_id, .description, \n",
    "    title:replace(value.title,'\"','')}\n",
    "WITH n, value\n",
    "CALL apoc.create.addLabels(n, case when value.type is null then [] else [apoc.text.upperCamelCase(replace(value.type,'\"',''))] end) yield node\n",
    "UNWIND split(value.source_id,\",\") as source_id\n",
    "MATCH (c:__Chunk__ {id:source_id})\n",
    "RETURN count(distinct n) as createdNodes\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_nodes.parquet',\n",
    "                     columns=[\"level\",\"title\",\"type\",\"description\",\"source_id\",\"human_readable_id\",\"id\",\"top_level_node_id\"])\n",
    "\n",
    "batched_import(statement, df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relationships\n",
    "\n",
    "statement = \"\"\"\n",
    "    MATCH (source:__Entity__ {title:replace(value.source,'\"','')})\n",
    "    MATCH (target:__Entity__ {title:replace(value.target,'\"','')})\n",
    "    // todo rel-type from source-target labels?\n",
    "    MERGE (source)-[rel:RELATED]->(target)\n",
    "    SET rel += value {.id, .rank, .weight, .human_readable_id, .description, text_unit_ids:value.text_unit_ids}\n",
    "    RETURN count(*) as createdRels\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_relationships.parquet',\n",
    "                     columns=[\"source\",\"target\",\"id\",\"rank\",\"weight\",\"human_readable_id\",\"description\",\"text_unit_ids\"])\n",
    "\n",
    "batched_import(statement, df)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import communities\n",
    "\n",
    "statement = \"\"\"\n",
    "MERGE (c:__Community__ {community:value.id})\n",
    "SET c += value {.level, .title}\n",
    "/*\n",
    "UNWIND value.text_unit_ids as text_unit_id\n",
    "MATCH (t:__Chunk__ {id:text_unit_id})\n",
    "MERGE (c)-[:HAS_CHUNK]->(t)\n",
    "WITH distinct c, value\n",
    "*/\n",
    "WITH *\n",
    "UNWIND value.relationship_ids as rel_id\n",
    "MATCH (start:__Entity__)-[:RELATED {id:rel_id}]->(end:__Entity__)\n",
    "MERGE (start)-[:IN_COMMUNITY]->(c)\n",
    "MERGE (end)-[:IN_COMMUNITY]->(c)\n",
    "RETURn count(distinct c) as createdCommunities\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_communities.parquet', \n",
    "                     columns=[\"id\",\"level\",\"title\",\"text_unit_ids\",\"relationship_ids\"])\n",
    "batched_import(statement, df)\n",
    "\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import communities\n",
    "\n",
    "statement = \"\"\"\n",
    "MERGE (c:__Community__ {community:value.community})\n",
    "// we can also extract findings as separate nodes\n",
    "WITH c, value, [f in value.findings | apoc.text.join([k in keys(f) | k+\": \"+f[k]],',\\n')] as findings\n",
    "SET c += value {.level, .title, .summary, findings, .rank, .rank_explanation, .id}\n",
    "RETURn count(distinct c) as createdCommunities\n",
    "\"\"\"\n",
    "\n",
    "df = pd.read_parquet(f'{GRAPHRAG_FOLDER}/create_final_community_reports.parquet',\n",
    "                     columns=[\"id\",\"community\",\"level\",\"title\",\"summary\", \"findings\",\"rank\",\"rank_explanation\"])\n",
    "\n",
    "batched_import(statement, df)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "/*\n",
    "cp ragtest/output/*/artifacts/*.parquet $NEO4J_HOME/import\n",
    "\n",
    "echo 'apoc.import.file.enabled=true' >> $NEO4J_HOME/conf/apoc.conf\n",
    "\n",
    "cd $NEO4J_HOME/plugins\n",
    "cp ../labs/*apoc*.jar .\n",
    "curl -OL https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/5.21.0/apoc-5.21.0-extended.jar\n",
    "curl -OL https://github.com/neo4j-contrib/neo4j-apoc-procedures/releases/download/5.21.0/apoc-hadoop-dependencies-5.21.0-all.jar\n",
    "cd ..\n",
    "bin/neo4j console\n",
    "*/\n",
    "\n",
    "// TODO - Load documents, text-chunks, claims, communities and connect them\n",
    "\n",
    "call apoc.load.parquet(\"create_final_nodes.parquet\") yield value\n",
    "// return keys(value), value limit 5\n",
    "return \n",
    "replace(value.type,'\"','') as type, value.id, value.level, \n",
    "replace(value.title,'\"','') as title,\n",
    "value.top_level_node_id, \n",
    "value.human_readable_id as nr,\n",
    "split(value.source_id,\",\") as sources,\n",
    "value.description \n",
    "LIMIT 5;\n",
    "\n",
    "/*\n",
    "{\n",
    "  \"source_id\": \"01e84646075b255eab0a34d872336a89,10bab8e9773ee6dfbb465bfa45794c34,28f242c45159426edb8589f5ca3c10e6,2f918cd94d1825eb5cbdc2a9d3ce094e,34c3d4a02c4a7e3b8ec57f41075aeeea,3fedcfeffb43c689a33ffa06897ad045,50160bdfa976f5b946c699722c81b412,535f6bed392a62760401b1d4f2aa5e2f,608db27bee139aaab8ded9989997d00a,680dd6d2a970a49082fa4f34bf63a34e,6968390fb201fda828835d2d1fd4e953,6ea022365de9ab0d226801de90139c8a,879b3fc36c9a2427cdb8d5d41b60e11b,972bb34ddd371530f06d006480526d3e,9e59af410db84b25757e3bf90e036f39,da3ca9f93aac15c67f6acf3cca2fc229,e8cf7d2eec5c3bcbeefc60d9f15941ed,f96b5ddf7fae853edbc4d916f66c623f\",\n",
    "  \"type\": \"\"ORGANIZATION\"\",\n",
    "  \"size\": 13,\n",
    "  \"id\": \"b45241d70f0e43fca764df95b2b81f77\",\n",
    "  \"title\": \"\"PROJECT GUTENBERG\"\",\n",
    "  \"level\": 0,\n",
    "  \"degree\": 13,\n",
    "  \"description\": \"Project Gutenberg is a pioneering organization dedicated to the free distribution of electronic works, with a focus on those not protected by U.S. copyright law. It was initiated by Professor Michael S. Hart and is supported by a network of volunteers and the Gutenberg Literary Archive Foundation. The organization's mission is to increase the number of public domain and licensed works freely distributed in machine-readable form, thereby promoting free access to literature and electronic works. Project Gutenberg owns a compilation copyright in its collection of electronic works, ensuring their accessibility while requiring compliance with specific copyright and distribution guidelines outlined in their license agreement.\n",
    "\n",
    "For over forty years, Project Gutenberg has been creating and distributing eBooks, offering a vast array of works in various formats, including 'Plain Vanilla ASCII'. Its collection includes notable titles like 'A Christmas Carol', available for free under a license that allows copying, giving away, and re-using with almost no restrictions. The organization operates globally, emphasizing copyright status and adherence to its license, which includes a system of royalty payments and refunds under certain conditions. Project Gutenberg's main search facility is accessible through its website, www.gutenberg.org, facilitating easy access to its extensive library.\n",
    "\n",
    "Project Gutenberg is committed to keeping its collection freely available for future generations, supported by donations and the efforts of its volunteer network. It promotes the creation, modification, and redistribution of eBooks, especially focusing on works that allow for free copying and distribution in the United States under specific terms. The organization is described as being focused on promoting free access to electronic works, ensuring that literature remains accessible to the public while keeping its name associated with shared works in compliance with its agreement.\",\n",
    "  \"top_level_node_id\": \"b45241d70f0e43fca764df95b2b81f77\",\n",
    "  \"human_readable_id\": 0,\n",
    "  \"__index_level_0__\": 0,\n",
    "  \"y\": 0,\n",
    "  \"x\": 0\n",
    "}\n",
    "*/\n",
    "create constraint chunk_id if not exists for (c:__Chunk__) require c.id is unique;\n",
    "create constraint document_id if not exists for (d:__Document__) require d.id is unique;\n",
    "create constraint entity_id if not exists for (c:__Community__) require c.community is unique;\n",
    "\n",
    "create constraint entity_id if not exists for (e:__Entity__) require e.id is unique;\n",
    "create constraint entity_title if not exists for (e:__Entity__) require e.title is unique;\n",
    "create constraint related_id if not exists for ()-[rel:RELATED]->() require rel.id is unique;\n",
    "\n",
    "\n",
    "call apoc.load.parquet(\"create_final_documents.parquet\") yield value\n",
    "return keys(value),value limit 1;\n",
    "\n",
    "// [\"__index_level_0__\", \"raw_content\", \"id\", \"title\", \"text_unit_ids\"]\n",
    "\n",
    "call apoc.load.parquet(\"create_final_documents.parquet\") yield value\n",
    "MERGE (d:__Document__ {id:value.id})\n",
    "SET d += value {.title, text_unit_ids:value.text_unit_ids, raw_content:substring(value.raw_content,0,1000)};\n",
    "\n",
    "call apoc.load.parquet(\"create_base_text_units.parquet\") yield value\n",
    "return keys(value),value limit 1;\n",
    "// [\"document_ids\", \"chunk\", \"n_tokens\", \"id\", \"chunk_id\"]\n",
    "\n",
    ":auto\n",
    "call apoc.load.parquet(\"create_base_text_units.parquet\") yield value\n",
    "CALL { with value \n",
    "MERGE (c:__Chunk__ {id:value.chunk_id})\n",
    "SET c += value {.chunk, .n_tokens}\n",
    "WITH *\n",
    "UNWIND value.document_ids as doc_id\n",
    "MATCH (d:__Document__ {id:doc_id})\n",
    "MERGE (d)<-[:PART_OF]-(c)\n",
    "RETURN count(distinct c) as chunksCreated\n",
    "} in transactions of 1000 rows\n",
    "RETURN sum(chunksCreated) as chunksCreated;\n",
    "\n",
    "\n",
    ":auto\n",
    "call apoc.load.parquet(\"create_final_nodes.parquet\") yield value\n",
    "call { with value\n",
    "    MERGE (n:__Entity__ {id:value.id})\n",
    "    SET n += value {.level, .top_level_node_id, .human_readable_id, .description, \n",
    "        title:replace(value.title,'\"','')}\n",
    "    WITH n, value\n",
    "    CALL apoc.create.addLabels(n, case when value.type is null then [] else [apoc.text.upperCamelCase(replace(value.type,'\"',''))] end) yield node\n",
    "    UNWIND split(value.source_id,\",\") as source_id\n",
    "    MATCH (c:__Chunk__ {id:source_id})\n",
    "    MERGE (c)-[:HAS_ENTITY]->(n)\n",
    "    RETURN count(distinct n) as created\n",
    "} in transactions of 25000 rows\n",
    "return sum(created) as createdNodes;\n",
    "\n",
    "\n",
    "call apoc.load.parquet(\"create_final_relationships.parquet\") yield value\n",
    "return keys(value), value limit 5;\n",
    "\n",
    ":auto\n",
    "call apoc.load.parquet(\"create_final_relationships.parquet\") yield value\n",
    "call { with value\n",
    "    MATCH (source:__Entity__ {title:replace(value.source,'\"','')})\n",
    "    MATCH (target:__Entity__ {title:replace(value.target,'\"','')})\n",
    "    // todo rel-type from source-target labels?\n",
    "    MERGE (source)-[rel:RELATED]->(target)\n",
    "    SET rel += value {.id, .rank, .weight, .human_readable_id, .description, text_unit_ids:value.text_unit_ids}\n",
    "    RETURN count(*) as created\n",
    "} in transactions of 25000 rows\n",
    "return sum(created) as createdRels;\n",
    "\n",
    "/*\n",
    "{\n",
    "  \"id\": \"b84d71ed9c3b45819eb3205fd28e13a0\",\n",
    "  \"target_degree\": 7,\n",
    "  \"rank\": 20,\n",
    "  \"source_degree\": 13,\n",
    "  \"weight\": 1.0,\n",
    "  \"source\": \"\"PROJECT GUTENBERG\"\",\n",
    "  \"description\": \"\"Project Gutenberg is responsible for releasing 'A Christmas Carol' as an eBook.\"\",\n",
    "  \"target\": \"\"A CHRISTMAS CAROL\"\",\n",
    "  \"human_readable_id\": \"0\",\n",
    "  \"text_unit_ids\": [\n",
    "    \"680dd6d2a970a49082fa4f34bf63a34e\"\n",
    "  ]\n",
    "}\n",
    "*/\n",
    "\n",
    ":auto\n",
    "call apoc.load.parquet(\"create_final_communities.parquet\") yield value\n",
    "// return keys(value), value limit 5;\n",
    "CALL { with value \n",
    "    MERGE (c:__Community__ {community:value.id})\n",
    "    SET c += value {.level, .title}\n",
    "    /*\n",
    "    UNWIND value.text_unit_ids as text_unit_id\n",
    "    MATCH (t:__Chunk__ {id:text_unit_id})\n",
    "    MERGE (c)-[:HAS_CHUNK]->(t)\n",
    "    WITH distinct c, value\n",
    "    */\n",
    "    WITH *\n",
    "    UNWIND value.relationship_ids as rel_id\n",
    "    MATCH (start:__Entity__)-[:RELATED {id:rel_id}]->(end:__Entity__)\n",
    "    MERGE (start)-[:IN_COMMUNITY]->(c)\n",
    "    MERGE (end)-[:IN_COMMUNITY]->(c)\n",
    "    RETURn count(distinct c) as created\n",
    "} in transactions of 1000 rows\n",
    "RETURN sum(created) as createdCommunities;\n",
    "\n",
    "// [\"level\", \"text_unit_ids\", \"relationship_ids\", \"id\", \"title\", \"raw_community\"]\t\n",
    "\n",
    ":auto\n",
    "call apoc.load.parquet(\"create_final_community_reports.parquet\") yield value\n",
    "CALL { with value \n",
    "    MERGE (c:__Community__ {community:value.community})\n",
    "    SET c += value {.level, .title, .summary, .findings, .rank, .rank_explanation, .id}\n",
    "    RETURn count(distinct c) as created\n",
    "} in transactions of 1000 rows\n",
    "RETURN sum(created) as createdReports;\n",
    "// [\"summary\", \"full_content_json\", \"level\", \"findings\", \"full_content\", \"rank\", \"id\", \"rank_explanation\", \"title\", \"community\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
